---
subject: "Welcome to the AI Snake Oil newsletter"
---
<img src="https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUwNTI4MTc1OTExLjMuZWFhNTA1NTBiNzgxODVkMi5vazdwYmsxYkBtZy1kMC5zdWJzdGFjay5jb20-IiwidSI6MzQ1MzUxNzI3LCJyIjoiYmxvZ3NAcmVwbGllcy5jYXRza3VsbC5uZXQiLCJkIjoibWctZDAuc3Vic3RhY2suY29tIiwicCI6bnVsbCwidCI6bnVsbCwiYSI6bnVsbCwicyI6MTAwODAwMywiYyI6ImZyZWUtd2VsY29tZSIsImYiOnRydWUsInBvc2l0aW9uIjoidG9wIiwiaWF0IjoxNzQ4NDU1MTUyLCJleHAiOjE3NTEwNDcxNTIsImlzcyI6InB1Yi0wIiwic3ViIjoiZW8ifQ.10Prw8BwoyZHGJo8d05Ei2t1rfMkCRVYm2eXZlN6DVg" alt="" width="1" height="1" border="0" style="height:1px !important;width:1px !important;border-width:0 !important;margin-top:0 !important;margin-bottom:0 !important;margin-right:0 !important;margin-left:0 !important;padding-top:0 !important;padding-bottom:0 !important;padding-right:0 !important;padding-left:0 !important;"><div class="preview" style="display:none;font-size:1px;color:#333333;line-height:1px;max-height:0px;max-width:0px;opacity:0;overflow:hidden;">Thank you for subscribing to AI Snake Oil</div><div class="preview" style="display:none;font-size:1px;color:#333333;line-height:1px;max-height:0px;max-width:0px;opacity:0;overflow:hidden;">͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­͏ &nbsp;   ­</div><table class="email-body-container" role="presentation" width="100%" border="0" cellspacing="0" cellpadding="0"><tbody><tr><td></td><td class="content" width="550"></td><td></td></tr><tr><td></td><td class="content" width="550" align="left"><div style="font-size: 16px;line-height: 26px;max-width: 550px;width: 100%;margin: 0 auto;overflow-wrap: break-word;"><img class="static-XUAQjT" src="https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6d267b36-4ea1-40c2-b41c-416073d16c63_256x256.png" width="40" height="40" alt="AI Snake Oil" name="AI Snake Oil" style="border-radius: 8px;box-sizing: border-box;background-color: rgb(255,255,255);margin: 0;max-width: 550px;border: none !important;vertical-align: middle;"><div class="post typography" dir="auto" style="padding: 32px 0 0 0;font-size: 16px;line-height: 26px;"><div class="body markup" dir="auto" style="text-align: initial;font-size: 16px;line-height: 26px;width: 100%;word-break: break-word;margin-bottom: 16px;font-family: Lora,sans-serif;font-weight: 400;"><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;margin-top: 0;"><span>You will receive new posts right here in your inbox. You can also go to the</span><a href="https://email.mg-d0.substack.com/c/eJxUkMuO4yAURL_G7GLx8A3MgsVs8hvWBa4dZAwWjxnl71tRL7qzLtWpo_LYaS_1ZbdKdPtPyZeTWLB3hRsERlboxSwAAiSjE2Nad8pUsVNYsf9KlVnY0yJtatNK6uClMEjqbhQudw3oYONBsmgll8BBGqHhjxCzmgkROAB32ggDQc7l0Jc7hJsWfu63wOc2XOvoj9mXk8W2vl3fLrbXQSzZZ-9Xm9TfST4m-cDYMh5UYvosXsOtvpznyLG_VsroEoVvxDVcih57LHmNwQrODeeKVetS2du08EpXitRmj70dI6U5U2dtuFBOjNn-TLL--eRoVN9ItYACoaVm_6z8CgAA__9ae3qr" rel="" style="color: #fd6752;text-decoration: none;"> website</a><span> to read the full archives and other posts as they are published.</span></p><h3 class="header-anchor-post" style="position: relative;--highlight-bg: rgba(var(--color-bg-accent-themed-rgb), 0.2);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>Get the book</strong></h3><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">AI is everywhere, and so is hype around AI. That's why we wrote a book on AI snake oil—to dispel hype, clear up misconceptions, and explain the limits of AI.&nbsp;</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>It is available via </span><a href="https://email.mg-d0.substack.com/c/eJxUkUGL3yAQxT9NvCWo0Y05eFgogZ566KW3MOokO8RoUNM_209fQi_d28A8fu8Hz0PDPZdPuxXE_oXR5xNZsG8jbDowtGJSRmkttGR4AsV1x4QFGoYV2n_f0Sj2YQ14lMZoNfOZ-20GLrUKkxMoFDwMspJLzbU0YtKzEMM4IIDmWnM3GWF0kEM-pssdwnWKn3sf-FBvVxv4Y_D5ZFTXx_Vxsa3cyKL9aO2q3fjeyaWTy-v1GuCEPzk9-U4uPxMc2P-g2L-XRht5gth_Tw1jpB2Tx_4bbRuW5-zkEq5OLvxtFlLNYvzFrtutPp_nnah9rpjARQz_qq_bRfLQKKeVghWcG85HVqyLea-d4gWvSFgHD60ed4xDwsbq7UI-gZIFqo9bpsja1wXuiuVBjkqPWkxyYr-t_BsAAP__OdyL2g" rel="" style="color: #fd6752;text-decoration: none;">Amazon</a><span>, </span><a href="https://email.mg-d0.substack.com/c/eJxUkc_OrCwMh69Gdhj-yOgsWHwbb8MUqNqIYADfydz9l5k3Jzln2fZp8zQ_Dw23XN52LYj8hdHnE1mwDw2rCQytHIdpMEYaxfAEisuGCQs0DAu0v6Z6GthupZl8mPRgcArai-fjKdUDHYTwBKEfKyOrhDLCqEmO5illr3sEMMIY4cZJTiaoPh_j5Q7pukGcGw-ir7erDfzR-3wyqsvH9eNiW7mRRbu3dtVO_9epuVOzy_moe776XLZOzdefVqdmIF4THMgzRf7aoXEojVbyBJFTahgjbZg8cg-Jh_zLUPuWjUMKfM8v3jL_oLztyAOtK5bvDpQfSoEnKPCGBKlTs5JaDY9xYNftFp_P807U3gsmcBHDr_91u0geGuW0ULBSiEkIzYp1MW-1G0TBKxLW3kOrxx1jn7CxeruQT6Bkgb4_ZYqs_RvjXbF8TurBaCNHNbIfq_4PAAD___jopfU" rel="" style="color: #fd6752;text-decoration: none;">Bookshop</a><span>, or your favorite bookseller. It’s one of </span><strong>Nature’s 10 best books of 2024</strong><span>.</span></p><div class="captioned-image-container-static" style="font-size: 16px;line-height: 26px;margin: 32px auto;"><figure style="width: 100%;margin: 0 auto;"><table class="image-wrapper" width="100%" border="0" cellspacing="0" cellpadding="0" data-component-name="Image2ToDOMStatic" style="mso-padding-alt: 1em 0 1.6em;"><tbody><tr><td style="text-align: center;"></td><td class="content" align="left" width="310" style="text-align: center;"><a class="image-link" target="_blank" href="https://email.mg-d0.substack.com/c/eJxUkUGL3yAQxT9NvCWo0Y05eFgogZ566KW3MOokO8RoUNM_209fQi_d28A8fu8Hz0PDPZdPuxXE_oXR5xNZsG8jbDowtGJSRmkttGR4AsV1x4QFGoYV2n_f0Sj2YQ14lMZoNfOZ-20GLrUKkxMoFDwMspJLzbU0YtKzEMM4IIDmWnM3GWF0kEM-pssdwnWKn3sf-FBvVxv4Y_D5ZFTXx_Vxsa3cyKL9aO2q3fjeyaWTy-v1GuCEPzk9-U4uPxMc2P-g2L-XRht5gth_Tw1jpB2Tx_4bbRuW5-zkEq5OLvxtFlLNYvzFrtutPp_nnah9rpjARQz_qq_bRfLQKKeVghWcG85HVqyLea-d4gWvSFgHD60ed4xDwsbq7UI-gZIFqo9bpsja1wXuiuVBjkqPWkxyYr-t_BsAAP__OdyL2g" rel="" style="position: relative;flex-direction: column;align-items: center;padding: 0;width: auto;height: auto;border: none;text-decoration: none;display: block;margin: 0;"><img data-attrs="{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c213d390-dd89-47f6-b90f-127b2738e7a7_964x1500.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1500,&quot;width&quot;:964,&quot;resizeWidth&quot;:310,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:&quot;https://www.amazon.com/Snake-Oil-Artificial-Intelligence-Difference/dp/069124913X&quot;,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null}" alt="" width="310" height="482.3651452282158" src="https://substackcdn.com/image/fetch/w_310,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc213d390-dd89-47f6-b90f-127b2738e7a7_964x1500.png" style="border: none !important;vertical-align: middle;display: block;-ms-interpolation-mode: bicubic;height: auto;margin-bottom: 0;width: auto !important;max-width: 100% !important;margin: 0 auto;"></a></td><td style="text-align: center;"></td></tr></tbody></table><figcaption class="image-caption" style="box-sizing: content-box;color: #777777;font-size: 14px;line-height: 20px;font-weight: 400;letter-spacing: -.15px;margin-top: 8px;width: 70%;padding-left: 15%;padding-right: 15%;text-align: center;"><em>AI Snake Oil book cover</em></figcaption></figure></div><h3 class="header-anchor-post" style="position: relative;--highlight-bg: rgba(var(--color-bg-accent-themed-rgb), 0.2);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>About the newsletter</strong></h3><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">While the book is about the foundational knowledge needed to separate real advances from hype, our newsletter offers commentary on new developments.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">All of the content on this newsletter is, and will be, publicly available. There is no paid subscription.</p><h3 class="header-anchor-post" style="position: relative;--highlight-bg: rgba(var(--color-bg-accent-themed-rgb), 0.2);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;"><strong>Housekeeping</strong></h3><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">If you can’t find the newsletter, check your spam folder. And please mark this address as ‘not spam.’ If the newsletter isn’t in your spam folder, either, you should look in the Promotions tab.</p><h3 class="header-anchor-post" style="position: relative;--highlight-bg: rgba(var(--color-bg-accent-themed-rgb), 0.2);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.375em;">Catch up on AI Snake Oil</h3><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">There are four main themes of our writing:</p><ol style="margin-top: 0;padding: 0;"><li style="margin: 8px 0 0 32px;"><p style="color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Resisting AI doom narratives</p></li><li style="margin: 8px 0 0 32px;"><p style="color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Debunking hype about AI’s capabilities and transformative effects</p></li><li style="margin: 8px 0 0 32px;"><p style="color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Understanding why AI evaluation is so tricky (which is one root cause of AI hype)</p></li><li style="margin: 8px 0 0 32px;"><p style="color: rgb(54,55,55);line-height: 26px;margin-bottom: 0;box-sizing: border-box;padding-left: 4px;font-size: 16px;margin: 0;">Helping AI policy stay grounded in the evidence.</p></li></ol><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Here are 40 essays from the newsletter on these four topics.&nbsp;We hope you will also </span><strong><a href="https://email.mg-d0.substack.com/c/eJxUkUGL3yAQxT9NvCWo0Y05eFgogZ566KW3MOokO8RoUNM_209fQi_d28A8fu8Hz0PDPZdPuxXE_oXR5xNZsG8jbDowtGJSRmkttGR4AsV1x4QFGoYV2n_f0Sj2YQ14lMZoNfOZ-20GLrUKkxMoFDwMspJLzbU0YtKzEMM4IIDmWnM3GWF0kEM-pssdwnWKn3sf-FBvVxv4Y_D5ZFTXx_Vxsa3cyKL9aO2q3fjeyaWTy-v1GuCEPzk9-U4uPxMc2P-g2L-XRht5gth_Tw1jpB2Tx_4bbRuW5-zkEq5OLvxtFlLNYvzFrtutPp_nnah9rpjARQz_qq_bRfLQKKeVghWcG85HVqyLea-d4gWvSFgHD60ed4xDwsbq7UI-gZIFqo9bpsja1wXuiuVBjkqPWkxyYr-t_BsAAP__OdyL2g" rel="" style="color: #fd6752;text-decoration: none;">buy the book</a></strong><span> — while the themes are largely the same, there is very little overlap in the contents.</span></p><h4 class="header-anchor-post" style="position: relative;--highlight-bg: rgba(var(--color-bg-accent-themed-rgb), 0.2);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.125em;"><strong>1. AI safety</strong></h4><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">We rebut alarmism about catastrophic AI risks and analyze flawed claims about how to make it safer.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0LvO4yAQBeCnMR0IsAmkoNgmr2ENZpwgcxOXjfL2q2iL3b8ezTmfzgEDn6V97NkQ6RvjURISb28rnMoTtEJvZlNKKEkwQYj7EzM2GOh3GP9dV7ORlxUo7jeP2ghxnpLfFLjNnQ48eK_vhpNgJZeKK2mEVnch2MoQQHGluNNGGOUlK5eu7hJu2Xh6Us9Zn64POC52lERC37_Wr8WONpFE-xqj9mX9tcjHIh_v95tB6BkuLCF-fxb5qIt8QKAdThwfGjrNZVCgqXiMtLZSsY0PqdPtR0lp5jA-O2ZwEf3fljpdDAeMUPIevBWcG85X0qyL5dmXjTesMWBnB4x-zRhZxkH6dL4kCNn-E5Hxc-zZsX0j102tSmipyW8r_wQAAP__IkeHQw" rel="" style="color: #fd6752;text-decoration: none;">AI safety is not a model property</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Trying to make an AI model that can’t be misused is like trying to make a computer that can’t be used for bad things. Safety depends on the context and the environment in which the AI model or AI system is deployed. So fixing AI safety by tinkering with models is unlikely to be fruitful. Even if models themselves can somehow be made “safe”, they can easily be used for malicious purposes. Safety guardrails must primarily reside outside the model.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0DvO3SAQBeDVmM4WD8-FFBRp7jasAcb-kXlYBnL17z6yUiSpR3POp-Ox01Hvb7vfRPOHkq-ZWLAvhTsERlbo1awAAiSjjDFtBxW6sVPYsP9zVWZlXxZ2t4IyRmsdyMvXqpUD83Ic5e52BBat5BI4SCM0_BBiUQshAgfgThthIMilnvpyp3DTyvMxB7604VpHfy6-Zhbb9lgfi-33IJbsV-9Xm9TPSb4n-f58PgvGVvCkGtPzM8n3Ncl3roHSjCkeJVPp83XXTr63GQ-MpXV2Dbf5mvMosX9vVNAlCn9KruFS9NhjLVsMVnBuOFfsti7Vo00rv-lKkdrisbdzpLQU6qwNF2rGWOxfEOv_bz0a3U-kWkGB0FKzX1b-DgAA__9RWIbr" rel="" style="color: #fd6752;text-decoration: none;">Model alignment protects against accidental harms, not intentional ones</a><br></strong><em>By Arvind Narayanan, Sayash Kapoor, and Seth Lazar</em></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Model alignment refers to modifying AI models to make them “safe”. We argue that this far more suited for curbing accidental harms (such as users unintentionally being shown offensive content) rather than intentional harms (such as curbing misuse by sophisticated adversaries). The hand wringing about failures of model alignment is misguided.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0DmO3DAQBdDTiJkIri1NwMBJX0MokiUNIS4CFwt9e0NwYE9cqP8fvoOOR6kfs1fE-cboSkLizUvCrj1Bwxe1Kq25FgQThLgdmLFCR79B_-8qV0W-jUKlnEe9M20BuIQvZflr35WS9rUrToIRTGimxcoX_cU5lRQBNNOa2WXlq_aClnO57MntpFg6Zs9oG7Z1cCd1JZHQtsf6WEyvA0k0371fbZK_JvGexPu-bwqhZTixhPj8TOJ9TeINcwotIviQj7lcmOeIvWOdwZbR5-YCuYbdXElp5NA_G2awEf3fmmvYGBz0UPIWvOGMrYxJUo2N5WiTYhWvGLBRB72dI0aasZM2rC8JQjb_SKT_XHs0rE-kVFpqvoiF_DbiTwAAAP__ffmHfA" rel="" style="color: #fd6752;text-decoration: none;">A misleading open letter about sci-fi AI dangers ignores the real risks</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">In March 2023, the Future of Life Institute released an open letter asking for a 6-month pause on training language models “more powerful than” GPT-4. Unfortunately, the letter's key premises relied on speculative risks and ignored the real risks of overreliance, centralization, and near-term security concerns.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkDuupDAURFeDM5Bt8MMEDibpbaBr-zbvCn-QP93Tux-hCWZeXDpVR-Wg4ZHLxzwL4vjG4HJE5s3XDE_lGRqxLnpRSijJMAKF_cCEBRr6Hdp_6awX9m3ktiIozeHLa279itI7Lja3LTAraTdGRnKpuJJarGoTYponBFBcKW5XLbTycsrnetlT2GHh8Rg9n2q3tYE7J5cjo7rfrreLaaUjC-a7tasO869BPgb5eL_fE1BNcGKmcDODfFyDfFAd4ZXJUzpG_N0ouUY5jc-S4wg0FoQQPuzqdnc5xp6ofXZMYAP6v0tXt4Ec3NRO3gjONeczK8aGfNRh4QWvQFgnB62ePYQpYWO1W58jUDL_rFj7eXivWO7KeVGzEqtc2cvIPwEAAP__25GIiA" rel="" style="color: #fd6752;text-decoration: none;">Is Avoiding Extinction from AI Really an Urgent Priority?&nbsp;</a><br></strong><em>By Seth Lazar, Jeremy Howard, and Arvind Narayanan</em></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Two months later, the Center for AI Safety released a twenty-two-word statement that simply said: "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war." We argue against alarmism about AI existential risk and instead advocate for building strong institutions that both reduce AI risks and put us in a position to better respond to future risks.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkEuu3CAURFdjZrb4PvCAQSa9DesCt_sh87H4pNW7j5wMkoxLp-qoPAx81faxz4a4vjH5mpEE-yXgqQJBy7Q0UimmOMEMMR0vLNhgYDhg_JMKI8m3NbtWXjKgu1RCyC-zc2bMbqjfuRdck2g55YoqbphWO2Ob2BBAUaWo04YZFfhWT325k7lF0vxaA936dH2APzdfM4n9uF1vFzvaRJLs9xhXX8SPhT8W_ni_3xvEXuDEGtPNLPxxLfwBDdd6YVmfdZYAI9ay5how9RX8mJDSh1zTHb7mPEscnwMLuIThz841XYr-N3bEYBmlhlJBmnWpvvoiacMrReybh9HPmdJWcJA-XagZYrF_ncj4_-7Zsd2VQiqhmOaa_LT8VwAAAP__FwaGfQ" rel="" style="color: #fd6752;text-decoration: none;">Are open foundation models actually more risky than closed ones?</a><span>&nbsp;</span></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Is AI so dangerous that we must stop its proliferation, such as by prohibiting the open release of model weights? In September 2023, we organized a one-day virtual workshop on responsible and open foundation models. Watch the workshop video </span><a href="https://email.mg-d0.substack.com/c/eJxUkL0OmzAUhZ8Gb7H8dwMZPLRqs1Vd2hld2xdiYTDCdlDevkJd2vUcnU-fjsdKcz4-djqIbicln1diwd41ThAYWdmbwQBIUIxWjGmcaaMDK4UR6z-tHgx7WTdJqZQLZBwpCjiBuYPywgj_AMQHi1YJBQLUIHt4SMk1J0QQAML1gxwgKJ6XfneLdJ0R63wLgpfmSkW_cJ9XFst4uV4uth6NWLKvWvfS6S-denbqeZ4n_-RWm6NrcCVY_avTz3env_Xw8-uvHw1-f_dsb270eV3bFutnpA1dovAXujeXosca8zbGYKUQgxCaHdalPJfOiIP2FKlwj7UsLSW-UWWluZBXjJvFWDZcKMfE6v_ftkLHhdQGNMhe9ext1Z8AAAD__3qlf5Y" rel="" style="color: #fd6752;text-decoration: none;">here</a><span>. In December 2023, we released a </span><a href="https://email.mg-d0.substack.com/c/eJxU0Tuu3iAQBeDVmM6IZ7ALijROFWUJ1gBj_8gYLB43uruP_psmaWd0jj7peOh4lvppj4o4_8bky40k2G8SDh0IWm7UorTmWhC8Iab9xIwVOoYd-j9fuSjysqs2B2jlYYHAAA2uRivwbDVeIl8ViVYwoZkWCzd65ZxKigCaac2cWfiig6DlMo-7uJsUu885MNqGax38RX25SWz72_q22F4HkmRfvT9tkt8nsU1ie0GkrUM-Sg0Uw5jE1mLHNokt4AEj9UlsR0xfF8GEnLmYxPajfGDNMZ_zrwfzvJWRA_RY8vyzBEyNPuEgz3C7L_c9cuyfO2ZwCcNfxjNciv4rscdgOWMLY5JU61I526RYxSdFbNRDb9dIiWbspA0Xyg0xW4gtw4UlJtL_X2M0rO9KqbTU3AhDPqz4EwAA__-ir5D_" rel="" style="color: #fd6752;text-decoration: none;">policy brief</a><span> on the topic, co-authored with Stanford researchers.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkDuu3CAUhldjOluAOddMQZFmtmEd4HgGmZcMZHR3HzkpktT_69PvsNOrXN_muIjmD0VXEjFvvlY8wDMyYlNaAQiQjBKGuL8o04Wd_I79H3XVir3Nw6pDrgTgNscPLYSiBwIX6DUeqL5YMJJL4CC12OAhxLIuhAgcgNtNCw1eLuXcqj2FnRRPr9nzpQ3bOrpzcSWx0Pab9WYx_RrEonn3Xtu0_pjkc5LPz-ezYGgZTyoh3plJPusknyXP_U1zKy5QxziHVNH1uRxzqZTno4zssYeSWR12dyWlkUP_3imjjeT_rNVhY3C_bXvwRnCuOV_ZZWwsrzYpflGNgdrisLdzxLhk6qwN60vCkM1fMtb_P300uu7KVcEKYpMb-2nkrwAAAP__JTOKFQ" rel="" style="color: #fd6752;text-decoration: none;">On the Societal Impact of Open Foundation Models.&nbsp;</a><br></strong><em>By Sayash Kapoor, Rishi Bommasani, Daniel E. Ho, Percy Liang, and Arvind Narayanan</em></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>We continued our in-depth analysis over the next three months. In February 2024, we released a </span><a href="https://email.mg-d0.substack.com/c/eJxU0Luu3CAUheGnMd0gLt4DKSjSzGtYG9ieg8zF4pLovH00SpPUS_r1aQWc9G79252d6PGbcmiFWHRPjSdERk6a3e4AEhSjgikfb6rUcVI8cP6zaruzLxcU-KfxO0oRtdfBRh01We2fBqIHw5JTQoEAZaWBH1JyzQkRBIDwxkoLUfF2mdtf0m-7KO9HFHwsPyaGi4dWWBrHx_qxuNkXsey-5rzHpn9u6rWpV-hn4WNiPVuPnOLa1KvdVB9nGZt6sXv5I7RSVk3z-6CKPlP8m7qXzyngTK0eKTophBVCs-58bu-x7aLTnRMNHnCOa-XMK002lo-tYKoO06h4UUuZzf8fXYP6J6l30CCNMuyXU38CAAD__6WifI8" rel="" style="color: #fd6752;text-decoration: none;">paper</a><span> that contributed a framework to analyze the marginal (that is, additional) risk of open foundation models compared to closed models and existing technology such as the internet. The paper has since been peer reviewed (ICML 2024).</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkD0O3CAUhE9jOhA_ZiEFRZq9hvUwDy8yGMtArL19ZKVI0s5oZj7NCh23en1dvBDpjXmtBUlwLwVRB4JOmNnOWgstCRZIednwwAs6hgX6P66yM_m4qKMFichfPM5SCR_ty9rIjeFR2mBIcpJLzbW0wugfQjDFEEBzrbk3VlgdJKu7Of0u_DTzstHAWRu-dVh3ttZCUlse1ofF9Wsgye7T-9km9XOS70m-7_tmkNoBO9aUn8wk3-ck3_2DNGcoQFOjdXRaI300DxttnzpyIOfwy1pLGUfq3wUP8BnDn5lz-JxW6KkeSwpOcG45V-RyPtetTTO_8MwJG1uht33kzA7spA0faoF0uL9IpP__9mh4PZVq1koLIw355eTvAAAA__-nWIaS" rel="" style="color: #fd6752;text-decoration: none;">The LLaMA is out of the bag. Should we expect a tidal wave of disinformation?</a></strong><a href="https://email.mg-d0.substack.com/c/eJxUkD0O3CAUhE9jOhA_ZiEFRZq9hvUwDy8yGMtArL19ZKVI0s5oZj7NCh23en1dvBDpjXmtBUlwLwVRB4JOmNnOWgstCRZIednwwAs6hgX6P66yM_m4qKMFichfPM5SCR_ty9rIjeFR2mBIcpJLzbW0wugfQjDFEEBzrbk3VlgdJKu7Of0u_DTzstHAWRu-dVh3ttZCUlse1ofF9Wsgye7T-9km9XOS70m-7_tmkNoBO9aUn8wk3-ck3_2DNGcoQFOjdXRaI300DxttnzpyIOfwy1pLGUfq3wUP8BnDn5lz-JxW6KkeSwpOcG45V-RyPtetTTO_8MwJG1uht33kzA7spA0faoF0uL9IpP__9mh4PZVq1koLIw355eTvAAAA__-nWIaS" rel="" style="color: #fd6752;text-decoration: none;">&nbsp;</a></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>When Meta's LLaMA language model was released in February 2023 (and subsequently, its model weights leaked in March 2023), researchers and commentators warned of a tidal wave of disinformation. We were skeptical. After all, the </span><em>supply </em><span>of misinformation has rarely been the bottleneck for successful disinformation operations. We wrote this essay to counter some of the early narratives that equated capable open language models with unending disinformation.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0T2O5CAUBODTmAwL8zNmAoJN-hrWA167kYFn8TOjuf2qtRvsxiWVvlIFGHhS-3HPhsi_MQcqyKL7UPA0kaHbdm21MZuRDAukfJxYscHAeMD4J1VWs5ezWoOyEFWI2kargrfBa_P5EZWRNliWnBTSCCPttpvPbVvVigBGGCP8bjdrolzp2m9_bX7Ropw8irVP3weEaw1UWOrH2_q2uNEmsuxeY9x9Ub8W-Vjk46rpfI1AeRafYKV2LvIRqA6sY5GPF33zQfxueEND_qTGxwt5xDxP5PTkfwemL-SQOFXeKSTIvGBMwO7pj0ClzJrGz4EVfMb4B3JPn1OAkageKbpNCCuEYs35TGdftGh454R9DTD6NXNeKw7Wp49UIFUHqVe4kFJm4_8_Zsf2rlTaKLPtcmdfTv4OAAD__3jWlBw" rel="" style="color: #fd6752;text-decoration: none;">How to Prepare for the Deluge of Generative AI on Social Media</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">We followed up on this essay with an analysis of what types of uses generative AI is most likely to find on social media. Our analysis highlighted the many benign uses; in terms of misuses, we argued that we should be far more concerned about non-consensual deepfakes than disinformation attacks. Unfortunately, it seems like we were right.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0LsO3CAQBdCvMZ0tHmYhBUWa_Q1rgLEXmYfFI9b-fWSlSFKP5t6j66DjUerX7BVxvjG6kpB48xKwS0_QMLXqVUomOcEEIW4HZqzQ0W_Q_7kKvZKP2Z21Su_-hbtgqFYtPFqlV8fdy4N0JBhOuaSSa6bkD8YWsSCApFJSqzTT0vOlnOqyJ7PTStMxe7q0YVsHdy6uJBLa9lgfi-l1IInm0_vVJvFz4u-Jv-_7XiC0DCeWEJ-fib-vib9bHx5zbzNUnMGFfMz9g6HOn5LwLvUk17CbKymNHPp3www2ov_Tcg0bg4MeSt6CN4xSTakg1dhYjjattOIVA7bFQW_niHHJ2Ekb1pcEIZu_ItL_H3s0rE-kWKWQTHFFfhn-OwAA___3rIga" rel="" style="color: #fd6752;text-decoration: none;">Students are acing their homework by turning in machine-generated essays. Good.</a><br></strong><em>By Arvind Narayanan</em></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Teachers adapted to the calculator. They can certainly adapt to language models. While that adjustment will be painful, it will force much-needed changes to education. (This essay was written before ChatGPT was released, and anticipated an issue that would soon become pressing.)</p><h4 class="header-anchor-post" style="position: relative;--highlight-bg: rgba(var(--color-bg-accent-themed-rgb), 0.2);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.125em;"><strong>2. Debunking AI hype</strong></h4><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Through our research and explanatory articles, we’ve shown what goes wrong when we fall for AI hype and how to avoid doing so.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkD2u3SAUhFdjOltwDIFXUKTxNix-zr0XGYMFh1hv95GVIkk5Gs3MpwmO8F3bt301xPnGHOqJLNofq3upyNAKLY1USihgeLqU9zcWbI4w7o7-cVcj2ccKrxUI_hWDF-YVpAQA4EZpwaPUWrBkgYPiCozQ6kuIZV3QOcWV4l4bYVSEpR768ofwk-Tne4586cN3cuFYQj1Z6vvD-rBYagNZth-iq0_rzwm2Cbb7vheXenEH1pSfzATbNcHm0hxcKZXmq2FMgWb64PwaNBrOftAEW6jniYU6u4bfHzFKou8di_MZ45-9a_icgqNUy56iFZwbzlfWrM_13SfJG145YV-Co36MnJeCxPrwsZ4uFfuXjdH_t4-O7alcpVqV0KDZLwu_AwAA__-pGolw" rel="" style="color: #fd6752;text-decoration: none;">AI cannot predict the future. But companies keep trying (and failing).</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>While generative AI dominates headlines, predictive AI is far more consequential and has been proliferating: AI for making decisions about individuals based on predictions about their future behavior. It is used in hiring, education, banking, criminal justice, and many other areas. We co-authored a </span><strong><a href="https://email.mg-d0.substack.com/c/eJxUkMuu2yAURb_GzIJ4mJgMGHSS37AOcOJ7ZAyIx63Sr6_STtrx1l5aWgEGHqW93ash3n5iCuVCFt1dw8tEhk5uq12NkUYxvIDSfmDGBgPjDuOfVduVfTlQDyHtaoR-xLvWAl-42hDt3b4eGAAZOSWUEUZZuZmHlFxzBDDCGOE3K62Jipdzq_6UflnFddyi4H36PiCcPJSLUd8_rh8XN9pEltzXGLUv-seinot61oaRwqBvvJU66KJfMKhkHjqvjXLAUTLHOBf1ZHX6PZTrmpnGe8cMPmH8i63TJwp_rjtFJ4WwQmjWnE_l6MsqGtZE2HmA0c-ZEs84WJ8-lgsoO6Ce4cRCiY3_686O7YPUq9FGbmpj3079DgAA__87g4IB" rel="" style="color: #fd6752;text-decoration: none;">paper</a></strong><span> showing that predictive AI in all these areas has a strikingly similar set of flaws. Any application of predictive AI should be treated with skepticism by default unless the developer justifies how it avoids these flaws.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0E2upSAQBeDVyEzDjyW-AYOe3G2YQsprRQQj0ObtvnPTg-43rtQ5X86Kld75_nbbTdQ_FNd8kghuMrhBEOSUHecRQIEWdCLH5U2JbqwUFqz_Xc08it3RNG2TNaC92ragEayxSqpp_FJEqEGw01KDBD0rC19KDWYgRJAA0ttZzRD0kA97-UP5bpTnuw9yKM2XiusxrPkUXJaP9WNx9W4kottrvUpnfnX61enX8zwDckl4UOb4-en06-r0q-7Ue-TaYwp9ebiue-9p5xR65P7mcoir-WXN59kS1--FEvpI4W_N1XzkFSvntHBwSspZSiNu52N-l26UN12RqQwr1nK0GIdEVZTmQz6Rk_tHEvXn2q3Q_Yk0IxhQVlvx2-k_AQAA__9oQ4Zu" rel="" style="color: #fd6752;text-decoration: none;">The bait and switch behind AI risk prediction tools</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Predictive AI vendors sell these tools based on the promise of full automation leading to cost efficiency, but when concerns are raised about bias, catastrophic failure, or other well-known limitations of AI, they retreat to the fine print which says that the tool shouldn't be used on its own. We discuss case studies in healthcare, welfare, and social services.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0LGu4yAQBdCvMZ2tAcyDFBTb5DesAeYlyBgsD2yUv19FW-y-ejT3Ht2InR7tevvvi2h-UYntIJH8l8ZvkwR5aVe3GiONEnRgLtuDKl3YKW3Y_7tqt4qnB6llAtCgbmm1Ot2-7Aogo7TqBto5kb0CZcAoJ625SbnohRANGAPBOulMUkvb7Rl2GaYVjsecYOERuGPcl9gOkXn7WD8W369Bovhn7ydP-tek7pO6v16vBTNX3Knl8vmZ1P2c1J1jptozd5752UZJ82CaMc_IM869tSLOEbbYjmPU3N8bVQyF0t-ac4SSI_bc6paTlwAOQIvLh9IePK1w0Vky8RKx8z5KWSp1wSOkdmCu_h9J9J9rD6brE6lXo420yorfXv0JAAD__ykBhYw" rel="" style="color: #fd6752;text-decoration: none;">Scientists should use AI as a tool, not an oracle</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>It is not just companies and the media who produce AI hype, but also AI researchers. A core selling point of machine learning is discovery without understanding, which is why errors are particularly common in machine-learning-based science. We published a </span><a href="https://email.mg-d0.substack.com/c/eJxU0D3O4yAUheHVmC7ogiGQgmKabMO6wI2DjMHiZ0bZ_SiaZr76SK8enYCD9to-7tWIbn8oh3oSi-6-4ktHRk4YZZXWQktGJ6a87VSo4aC44fhvXa1ib0cUjAV_N8pbiNEoGWUwD0uKXhAeiiUnQWrQ0gqjH0LwlROiBq3BGyusjpLXw1z-EH5RcO63CLxP3weGg4d6stS3r_VrcaNNYtm9x7j6sv5a5HORz0ZXq3GG5DPx0PnVUgk0auEU5yKf7Jp-C_U8Z0njs1FBnyn-a13T5xRwpFq2FJ0AsAAra87nuvdFQaMrJ-o84OjHzJkXGqxPH-uJqThMveBBNWU2fl46O7VvclV61cJIw347-TcAAP__qXV9sQ" rel="" style="color: #fd6752;text-decoration: none;">paper</a><span> compiling evidence revealing that an error called leakage — the machine learning version of teaching to the test — was pervasive, affecting hundreds of papers from dozens of disciplines. We think things will get worse before they get better.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0LvO4yAQhuGrMV0sDp5ACoptchvWAGMHmYNlYKPc_SraYvevR_Pq0eex016vj90uotubkq-ZWLB3hRsERlboxSwAAiSjjDGtOxW6sFNYsf93VWZhL8vD4g2pQHfn7hsYWDYK94fWXhnwj41FK7kEDtIIDQ8hZjUTInAA7rQRBoKc66FPdwg3LTzvt8DnNlzr6I_Z18xiW7_Wr8X2axBL9tX72Sb1a5LPST7f7_eMsRU8qMb0_Znk85zkE-OteUyx7Lf86a_GzuFWX3MeJfbPSgVdovA3eg6Xoscea1ljsIJzw7lil3Wp7m1a-EVnitRmj70dI6W5UGdtuFAzxmL_AVj_ue1odH2TagEFQkvNflv5JwAA__83p4Dm" rel="" style="color: #fd6752;text-decoration: none;">AI scaling myths</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>We argue that predictions of bigger and bigger AI models leading to AGI, or artificial general intelligence, rest on a series of myths. The seeming predictability of scaling is a misunderstanding of what research has shown. Besides, there are signs that LLM developers are already at the limit of high-quality training data. And the industry is seeing strong </span><em>downward</em><span> pressure on model size. While we can't predict exactly how far AI will advance through scaling, we think there’s virtually no chance that scaling alone will lead to AGI.&nbsp;</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkLuO3SAURb_GdEY8zMAUFGnub1gHOL6DzMPiEWv-PrJSJFMv7aWl7WHgu7ZvezTE9cbka0YS7IeEQwWCluvNbEpxJQhmiGl_Y8EGA8MO4z8qzUa-rBFeic_DbBg0oAFjPvgh1cG8BmdkINEKJhRTwnCtPjmnkiKAYkoxpw03KghaT325k7tlY_m9Bkb7dH2AP6mvmcS-P61Pix1tIkn2a4yrL_LXIl6LeN33TSH2AifWmJ7NIl7XA2JKK8R1NCj9qC2vCW5yTbf7mvMscXzvWMAlDH_N13Qpehixlj0GyxkzjEnSrEv13ZeNNbxSxE49jH7OlGjBQfp0oWaIxf6rIOPnwbNje5RyU1JxLTT5bcWfAAAA__-KyoKZ" rel="" style="color: #fd6752;text-decoration: none;">Will AI transform law?</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>We co-authored a </span><a href="https://email.mg-d0.substack.com/c/eJxU0b2u3CAUBOCnMZ0RP-bat6BI49ewDofjXWQMiJ-stsmzR6s0ufVIM580CJ0eub7tWYnmF0XMNzFvvzScxjOycl22xRhpFKMbQjwelKhCJ39A_y_V28KeVqN2ir7FYnBTGuHEL43OCKecPMGvLFgllBFGbXI131JyzQnACGOEWze5Ga94vtbiLummRdyP2Qvehmsd8OKYbxba8bF-LLbXQSzaZ--lTfrXpPZJ7a_Xi2PjpYaE1HPi5Mek9j8N3tCe16T2AoVqm9SOFeN8Qcm5zk9KnmrLaU5Q4Q0JEi_-ZGW4A_N9jxT6-6AELpL_t1yGiwGhh5yO4K0UYhNCs2pdzI82LaJSiYEaR-jtGjHyRJ214Xy-ISQLoSW4KIfI-s8DRqP6qdSL0UauamW_rfobAAD__28ujyU" rel="" style="color: #fd6752;text-decoration: none;">paper</a><span> on the promises and pitfalls of AI in law in which we categorized legal applications of AI into three rough areas. Our key thesis is that the areas that would be most transformative if AI were successful are also harder for AI as well as more prone to overoptimism due to evaluation pitfalls. In short, the hype is not supported by the current evidence.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0MvOpCAQBeCnkZ2Ei4i9YDGbfg1TQGkTEQyXMf32EzOLmX9dqXO-HAcN91y-ZiuI443R5ROJN7OETXmChutpmZTiShA8IcR1x4QFGvoV2n9XuUzkY0BbnC1_OTXrWTPh3PziTL2c01ZaK0kwggnFlFi4Vi_OqaQIoJhSzOqFL8oLmg992YPbYWLnPnpGa7e1gTuoyycJdX2sj8W00pFE82ntqoP8NYj3IN73fVMINcGBOcTnZxDvaxBvDPunIabxCm2DGOvY8mjxhoJj3saQyNXt6vJ59hTad8UENqL_23J1G4ODFnJagzecsYUxSYqxMe91mFjBKwas1EGrR4-RJmykduvzCSGZfyLSfo7dK5YnUk5KKq6FJr-N-BMAAP__h92Guw" rel="" style="color: #fd6752;text-decoration: none;">Eighteen pitfalls to beware of in AI journalism</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Journalists often uncritically repeat AI companies’ PR statements, overuse images of robots, attribute agency to AI tools, or downplay their limitations. We noticed that many articles tend to mislead in similar ways, so we analyzed over 50 articles about AI from major publications, from which we compiled 18 recurring pitfalls. We hope our </span><a href="https://email.mg-d0.substack.com/c/eJxUkL3O4yAURJ_GdEGAwZCCYpu8hnWBGwcZA-JnozT77Kvoa3a7kUaaOToeBh6lfeyzId7emHy5kAS7rfBUgaDlWhqpFFeC4AUx7QdmbDAw7DD-aVcjyctKr81q5GYEkxLV_e6VFgrYhmITWmwkWsGEYkoYrtWdc7pSBFBMKea04UYFQcupqzu5WyS7jltgtE_XB_iT-nKR2Pcv65fFjjaRJPsao_Zl_bWIxyIe7_eb-k5ri9njKJlimIt4_Onwgf46F_GAeHt9Kv6khrW0EfNxq3E8IaVOa3iSOt3uy3XNHMdnxwwuYfg5rNOl6GHEkvcYLGfMMLaSZl0qR18ka1hTxE49jH7OlGjGQfp0oVwQs4XYM5xYYiLjf--zY_tOrlKtimuhyW8r_gYAAP___wuJtQ" rel="" style="color: #fd6752;text-decoration: none;">checklist</a><span> can help journalists avoid hype and readers spot it.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkMHO3CAQg58m3BIByfywBw697GtEA8xmUUiIYOjqf_sqaqW2Z8v2Zwdk2kr9dq9KNH4oh3KQiO5rxhdEQU6ZxS4ACrSgA1NeNzqpIlNckf9RZ7uIt5PmQRaClIt8mIiwvL5AR6st4WPWwYvktNQgQVtl4KHUNE-ECBJAemOVhainspvL78oPizy2Mcqpdd8Ywz6FcojU1pv1ZnFcO4ns3sxXG-Yfg34O-vn5fCZM7cSdSsq3Z9DPa9DP8EbeLh5TG3H0Pef2Tjz-2VPq6DuLq_s1lOPoZ-LvlU70meLvnqv7nAJyKueaolNSWilnUZ3PZWvDIitdOVGbAnLbe87TSSxa97EcmE73l0nw_3f3RvWOnBeYQRltxE-nfwUAAP__P8SHVg" rel="" style="color: #fd6752;text-decoration: none;">ChatGPT is a bullshit generator. But it can still be amazingly useful.</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">The philosopher Harry Frankfurt defined bullshit as speech that is intended to persuade without regard for the truth. This is just what large language models are trained to do. But there are many tasks where they can be extremely useful despite being prone to inaccurate outputs.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0LvO3CAQBeCnMZ0twGZhC4o0-xrWAIMXmYvFJat9-8hKkfz1aM75dCx0PEr9al8R5w9GWxISpx8reOEIaiY3tQnBBCeYIMT9wIwVOrod-n_XVW3krZUAYFTC5jclueFe-fXpH1SiQ0v5gwTNKRdUcMWkeDK2rAsCCCoENVIxJRxfyikvczIzbTQds6NLG6Z1sOdiSyKh7bf1tuheB5Ko371fbVp_Tfw18dfn81kgtAwnlhDvn4m_rom_Eth3yDhHhJpDPubQ5tHQjzj7UucE-UuuYXZbUho59O-OGUxE97fnGiYGCz2UvAenGaWK0pVUbWI52rTRilcM2BYLvZ0jxiVjJ20YVxKErP-ZSP8592hY78h1E6tgkkvyW_M_AQAA__-wSYek" rel="" style="color: #fd6752;text-decoration: none;">ML is useful for many things, but not for predicting scientific replicability</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">A prominent paper claimed that machine learning could be used to predict which studies would replicate, and one of the authors has suggested that such predictions could be used to inform funding decisions. We co-authored a response describing the many flaws in the model presented in the paper, and argue that attempting to predict replicability is fundamentally misguided.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkLvO3CAUhJ_GdLa4LmxBkWZfwzrAWS8yBotLrH37yEqR_PWnmfk0HjpupX7tuyLOFyZfDiTBPgS8VSBomZZGKsUUJ3hATOuGGSt0DCv0_6gwknxseDAMXCqDWgfHhHq6NzPc8PAE-XwgiZZTrqjihmn1ZGwRCwIoqhR12jCjAl_Krk-3MzdJemxzoEsbrnXw--LLQWJbb9fbxfY6kCT76f1sk_g18dfEX9d1LRBbhh1LTHdm4q_zBp_vDBXngHjOCaHmmLe5o__kksoWW2_kHG715ThGjv27YgaXMPzdOYdL0UOPJa8xWEapoVSQal0qW5skrXimiG3x0Ns-UloydtKGC-WAmO0_J9J_3j0a1rtSSCUU01yT35b_CQAA__-tZIfI" rel="" style="color: #fd6752;text-decoration: none;">Why are deep learning technologists so overconfident?</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Deep learning researchers have proved skeptics wrong before, but the past doesn't predict the future.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkLuu3CAYhJ_GdLa4mANbUKTZ17B-4F8vMjdxyerk6SMrRZJiqtHMfBoHA8_Svs2rIa4fjK4kJN58CXhJT9AwtetdSiY5wQQhHidmbDDQHzD-cYXeydt8afVg3KJmoJhlTlBmuXzslnN8yZcnwXDKJZVcMyUfjG1iQwBJpaRWaaal51u5VLUXs8tO07l6uvVp-wB3ba4kEvpxs94sZrSJJJr3GLUv4sfCnwt_fj6fDULPcGEJ8c4s_FlvYakR1wuxrpDHu5VaUmn1HX6FfK4QSJ32cCWlmcP4PjCDjej_rNRpY3AwQslH8IZRqikVpBkby9mXnTasMWDfHIx-zRi3jIP0aX1JELL5S0TG_2fPju2uFLsUkimuyE_DfwcAAP__l5aHIg" rel="" style="color: #fd6752;text-decoration: none;">People keep anthropomorphizing AI. Here’s why.</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Companies and journalists both contribute to the confusion.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0M2u3CAMBeCnCbsg_jxhFiy6mdeITPDkokCIAtxo3r6K2qrt2rL9nbNgo7WcH_c-icaL0lIyseAeGt8QGDk5GWsAJChGGWOaV9rpxEZhxvbPVFvDvpzwRoC0-klBe_MGpOfDyAeYNyprvGLRKaFAgLJygqeUXHNCBAEg_GSlhaB42abDb9IPRuR1DILX7mvDZeNLySzW-bbeFtfOTiy5r9aOOugfg3oN6nVdF8dYd9yoxHTvDOp1DOr1Wx6_acQ45hIo1fFPnBEjO7qfl5Jz32P7zLSjTxR-PTm6T3HBFss-x-CkEFYIzU7nU1nrYMRJR4pU-YKtbj0lvlNjtftQMsbd_QWx9n_XvdJ5n9QGNMhJTezbqZ8BAAD__44Thgs" rel="" style="color: #fd6752;text-decoration: none;">Generative AI models generate AI hype</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Over 90% of images of AI produced by a popular image generation tool contain humanoid robots.</p><h4 class="header-anchor-post" style="position: relative;--highlight-bg: rgba(var(--color-bg-accent-themed-rgb), 0.2);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.125em;"><strong>3. AI evaluation</strong></h4><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Evaluating models on benchmarks was good enough in classic ML, even if imperfect. But in the LLM era, this has broken down. Chaos reigns.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0LuO3SAYBOCngW4RF2OzBUUav4b1A7-9yFwsLnu0TZ49OkqT1CPNfBoPA6_afuzZED9emHzNSINdFZw6ULRiW8yitdCSYoaYjgsLNhgYDhj_pMos9MvCCSv_XM8lhE2vclHGKW6M1wrDegak0UouNdfSiE1_CsEUQwDNteZuM8LoIFm9t8fdwpGF5-sjcNan6wP8zXzNNPbjbX1b7GgTabJfYzydqF9E7kTur9eL-c6eFovHUQvDMIncf0P7jiUUIvcB6e5E7vgNacKI5TpSyv3IseAZMQUid_pMd_ia8yxx_BxYwCUMfxef6VL0MGItRwxWcG44V7RZl-rVycIbPiliZx5Gv2dKrOCgfbpQM8RiIfYCN9aY6Pj_-NmxvSvVopUWm9zot5V_AgAA__-bHIvM" rel="" style="color: #fd6752;text-decoration: none;">Evaluating LLMs is a minefield</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">We released talk slides showing that current ways of evaluating chatbots and large language models don't work well, especially for questions about their societal impact. There are no quick fixes, and research is needed to improve evaluation methods.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">The talk is based on many of the essays from the newsletter, including the following three.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0DkO2zAQBdDTiJ0ErhZTsEjjawhDciQT4gYuMXz7wEiRpB7M_w_fwcCrtI85G-L6xuhKQuLNQ8CpPEHDdqmlUkxxgglCPC7M2GCgP2D8cxVakpcRSiE_pX3o82SeAz4cZV57tFpo5xgJhlOuqOKa7eoHY5vYEEBRpajdNdPK863ce7U3s4uk6Vo93fq0fYC7N1cSCf34Wr8WM9pEEs1rjNoX8XPhz4U_3-_3BqFnuLGE-P1Z-LMu_HnVscoVsl9rKyf2HkqGuFrM7pWg3Z3UaQ9XUpo5jM-BGWxE_6elThuDgxFKPoI3jFJNqSDN2FiuvkjasMaAfXMw-j1j3DIO0qf1JUHI5q-IjP_Hnh3bN1JIJRTb-U5-Gf47AAD__y7Qh1Y" rel="" style="color: #fd6752;text-decoration: none;">GPT-4 and professional benchmarks: the wrong answer to the wrong question</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">GPT-4 reportedly scored in the 90th percentile on the bar exam. So there’s been much speculation about what this means for professionals such as lawyers. But OpenAI may have tested on the training data. More importantly, it’s not like a lawyer’s job is to answer bar exam questions all day. There are better ways to assess AI’s impact on professions.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0E2upSAQBeDVyEwDCC0OGPTkbsMUUtdH5MdA8czdfcf0oPuNK3XOl7MD4VHqx74r4nhj3EtC5u2vGd7aM7RiUUZpLbRkmCDE7cCMFQj9BvTfdTaKfVlEuSppUK9ozM6Vw0W-HayrBK3W1bBgJZeaa2nEolchpnlCAM215m4xwmgvp3IulzuFGxRPx-j51LprBPs57SWx0LbH-lgs1Y4s2i-iqw3z70G-Bvm673uC0DKcWEJ8fgb5ugb5Cm08LhrVeCBRyMd4l9pwLN9YRwoJ2dXdtpeUeg702TCDi-j_llzdxbADhZK34K3g3HA-s2pdLEcbFK94xYBt2oHa2WOcMhJr3fmSIGT7D8To59a9YX0iZ6VnLRa5sG8r_wQAAP__w6SGFA" rel="" style="color: #fd6752;text-decoration: none;">Is GPT-4 getting worse over time?</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>We dug into a July 2023 paper that was being interpreted as saying that GPT-4 has gotten worse since its release. While we confirmed that the model’s </span><em>behavior</em><span> had drifted over time, there was no evidence that its </span><em>capability</em><span> had degraded. We explain why this might nonetheless be problematic.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0E2OhCAQBeDTyE4CKA29YDEbr2EKqFYiiuGnTd9-YmYxM-tKvfflOai4pPwxr4zYXxhd2pF48xjgJT1Bw9WoRym5FAR3CHFe8MAMFf0M9c910CNZjX-CezKmNAPpNXhwQlqh_MMK_ngJS4IRTEgmheZKPjmnA0UAyaRkVmmupRc0beq0G7fdyPal94yWZksFt1GXdhLKfFtvi6m5IYlmrfUs3fDViakT03VdFEI5YMMU4v3TiensxOQTlt6tUJez9iu8sYc-BosZYm8DFHI2O7u07-0I9TPjATai_yk5m43BQQ3pmIM3nDHN2ECysTEtpRtZxjMGLNRBLVuLkR5YSWnWpx3CYX5BpP7fuhXMd-QwykFyJRR5G_EdAAD__z6MhnY" rel="" style="color: #fd6752;text-decoration: none;">Does ChatGPT have a liberal bias?</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">An August 2023 paper claimed that ChatGPT has a liberal bias, agreeing with Democrats the vast majority of the time. The media reported on the paper unquestioningly and the findings were red meat for Reddit. In another instance of real-time, public peer review, we exposed a long list of fatal flaws in the paper’s methods. It is possible that ChatGPT expresses liberal views to users, and the question merits research, but this paper provides little evidence of it.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0L2u3CAQBeCnMZ0Rf3PtLSjS7GtYA4x9kTFY_GS1bx-tUiS3Hs05n47HTkepb7tXovlFyZeLWLBfGncIjKxczGoAJChGF8a0HZSpYqewYf_vqlfDvq0TRnhvFmdCeDhJRqtdrw_x-AJQOwGLVgkFAtQqF3hIyTUnRBAAwi2rXCEoXs7ldqd0kxHXMQfB23Ctoz-5LxeLbftYPxbb6yCW7Hfvd5v0r0k9J_V8vV4cY8t4Uonp8zOp5z2pJ8Y5EQaqrmANbcZKcy5zKvmgOo9G-0jsHm7z5bpGjv29UUaXKPwtuodL0WOPJW8xWCnEKoRm1bpUjjYZUelOkRr32Ns5UuKZOmvDhXJhzPYfivWfe49G9ROpDWiQi1rYb6v-BAAA___0UIfC" rel="" style="color: #fd6752;text-decoration: none;">AI leaderboards are no longer useful. It's time to switch to Pareto curves.</a><br></strong><em>By Sayash Kapoor, Benedikt Stroebl, Arvind Narayanan</em></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">We present new research showing that one-dimensional evaluations that focus on accuracy alone are misleading, and advocate for Pareto curves can help visualize the accuracy-cost tradeoff.&nbsp;</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0LvOnDAQBeCnwR3IF7xA4SLNvgYa41nWwjfZ46B9-2iVIvnr0Zzz6RxAeOb6Ma-KON4YjhyROfNQ8NKOoRHLvM5aCy0ZRvBhPzFhBUK3A_13VevM3sZuXONLbXLeQONDIYA6Xpt9CMtRomDeSC4113IVi96EmNSEAJprze2yilU7OeVrKfYSdph5PEfHp9ZtIziu6ciR-bZ_rV-LodqRBfMmKm1Qvwb5HOTzvu8JfEtwYfbh-zPIZxnkM-E9FihYR_AjnJiojfQGGiMQYWWl2_3IMfbk6bNjAhvQ_e0o3QZ_APmcdu-M4HzlXLFqbMhnG2ZesQSPbTqA2tVDmBISa926HMEn88_D6OfUvWH9RqpZKy0WubDfRv4JAAD__9u8hlY" rel="" style="color: #fd6752;text-decoration: none;">New paper: AI agents that matter</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>AI agents are systems that use large language models under the hood to carry out complex tasks such as booking flights. But on top of the difficulty of LLM evaluation, it turns out that agent evaluation has a bunch of additional pitfalls that has led to overoptimism. Many agents do well on benchmarks without being useful in practice. In a new </span><a href="https://email.mg-d0.substack.com/c/eJxUkE3OpCAURVcjsyLwkEIHDHpS2zAPeFpEFMNPddfuO6Yn_c1ucpOTk-Ox0ZbL166F6PGbks8HsWCfClcdGFlpxmnUWmpgdGBMy0YnFWwUFmz_vWoa2duuT0-TRD2vxgmYjQFwXgYZ5hBIKWDRggAtNEzS6FlKrjghaqG1cGaSkw7A824ut0s3jOLYHkHw2l1t6Hfu88FiXW7X28W20okl-27tqoP6NcBrgBeWP_HDc9nu7eoALxiF4UJqAezqbvH5OPoZ23ehE12i8I9zdZeixxbzucRgpRCTEIoV61Le6jCKQleKVLnHVveeEj-psdpdyAfG02KsJ-6UY2LtZ85eqdxINWqlpQHDPhb-BgAA__8dTXmZ" rel="" style="color: #fd6752;text-decoration: none;">paper</a><span>, we identify the challenges in evaluating agents and propose ways to address them.&nbsp;</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkEkO2yAUhk9jdrYYA1mw6CbXsB7wkiAzWAy1cvsq7aLt-p8-_R4Gvmr72GdDXC9MvmYkwd4EPFUgaJmWRirFFCeYIab9hQUbDAw7jH9UYSR5WyODd0xqyZkCvD0Nw9tdCslBO3G_aRItp1xRxQ3T6s7YJjYEUFQp6rRhRgW-1UOf7mBukTS_1kC3Pl0f4I_N10xi37-sXxY72kSS7HuMsy_ix8IfC39c17VB7AUOrDF9Mwt_nAt_vOu1jgaln9CwjBUars86S4ARa1lzDZjIOd3ua86zxPHZsYBLGP4MndOl6H-b9xgso9RQKkizLtVXXyRteKaIffMw-jFT2goO0qcLNUMs9i8UGf__PTu2b6WQSiimuSY_Lf8VAAD__9EMh9s" rel="" style="color: #fd6752;text-decoration: none;">How Transparent Are Foundation Model Developers?</a><br></strong><em>By Sayash Kapoor</em></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>We present the </span><a href="https://email.mg-d0.substack.com/c/eJxU0Duu3SAUheHRmO4iHt4BFxRpzjSsjdn2ReZh8Uh0Zh8dpUnqJf36tA4cdNX2dmcj-vpN6aiZWHA_NJ4QGDlpVrsCSFCMMsa0X1So4aCw4_hn1XZl345WYwzgJmAL5xasJtj0aq2nsAVliEWnhAIBykoDm5Rcc0IEASC8sdJCULze5vG39Msq8vUVBO_T94HHzY-aWez7x_qxuNEmseS-x3j6on8u6rWo19HOzPvActYWOIW5qNeZR2TP9PtRc54ljvdOBX2i8LfxTJ_igSPWssfgpBBWCM2a86lefVlFoydF6vzA0e-ZEi80WJ8-1IyxOIy94E01Jjb-v3J2ap-kXkGDNMqwX079CQAA__9H7HpO" rel="" style="color: #fd6752;text-decoration: none;">Foundation Model Transparency Index</a><span>, a project that aggregates transparency information from foundation model developers. It helps identify areas for improvement, push for change, and track progress over time. This effort is a collaboration between researchers from Stanford, MIT, and Princeton. We assessed 10 major developers and their flagship models on the index’s 100 indicators, finding that the average score is just 37 out of 100.&nbsp;</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0LvO3CAQBeCnMZ0tLibwFxRp9jWsAebfHRkD4pLVvn1kpUhSj-acTyfAwGdpH_fdENc3plAuZNH9UPCtI0MnzG53rYWWDC-gdDwxY4OB8YDxz1XZnb2cj16B2oUVkUuzhy-OPBovhbBWBikYOcml5lpaYfSXEJvaEEBzrbk3Vlgd5VZOU_0p_LLz67lGvvXp-4BwbqFcjPpxW2-LG20iS-41Ru2L-rnIxyIf7_d7A-oZTiyU7p9FPuoiH6ViBuprLYkCYV9flCO2tWFtJc5APiGr0x-hXNfMND4HZvAJ45-iOn2iAINKPig6wbnlXLHmfCrPvuy8YU2EfQsw-jlT2jIO1qeP5QLK7i-Kjf_3nh3bHal2rbQw0rBfTv4OAAD__-1Yh_s" rel="" style="color: #fd6752;text-decoration: none;">OpenAI’s policies hinder reproducible research on language models</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">OpenAI discontinued a model with three days’ notice — a model that was widely relied upon by researchers. While the company reversed course after an outcry, it highlights the systemic risks inherent in reliance on privately controlled research infrastructure.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0D0OnDAQBeDT4A7kH2bNFi7S7DXQ2B5YC2MT_2S1t49QiiT1aN779Bw22nP5mq0QjR-KLp_EvHko3MAzMkLPywwgQDI6McR1p0QFG_kV2z9XtczsbR7eztpJvXH-fGrlvZMPQI8OFrCbFCwYySVwkIvQ8BRiUhMhAgfgVi9iAS-nfOjLHsIOMz_30fOpdlsbumNy-WShrrf1tphWOrFo3q1ddVA_Bvka5Ovz-UwYasKDcoj3zyBf1yBfPzumFrZvSPvo3tj2q9Vxp-SpjDZgZVe3q8vn2VNo35US2kj-T8nVbQwOW8hpDd4IzhfOFSvGxrzXYeaFrhioTg5bPXqMU6LGarc-nxiS-Qti7f-te6VyR6oZFAgtNftl5O8AAAD__4K-hwY" rel="" style="color: #fd6752;text-decoration: none;">Quantifying ChatGPT’s gender bias</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">ChatGPT shows a strong gender bias in some cases, such as arguing that attorneys cannot be pregnant. We quantify this behavior using a dataset called WinoBias. One important caveat is that we don’t know how often real users encounter this behavior.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0DkO2zAQBdDTiJ0IbmMxBYs0vobAZSwT4iJwieHbB0KKJPVg_n_43g48avuaV0NcP5h8zUiCeUj7gkDQ8E1pBcBBEMw2pv3Ags0ODLsd_1ylVuRtlN2Y9iAUKCYfQjKEB2ihwkN5CU6RaAQTwEBovsEPzqmkaC0wAOY2zTUEQeu5Xe7kblEsH2tgtE_Xh_Un9TWT2PfbelvMaBNJMu8xrr7In4t4LuL5-Xyojb3YE2tM988intcinrGMVsP0sRzreOPa8FVb7qt_oz9T7INc0-2-5jxLHN8di3UJw5-Wa7oUvR2xlj0GwxnTjEnSjEv16ItiDa8UsVNvRz9nSrTgIH26ULONxfwVkfH_2LNjuyOlAgl8Exv5ZcTvAAAA__90mIYs" rel="" style="color: #fd6752;text-decoration: none;">Introducing the REFORMS checklist for ML-based science</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>To minimize errors in machine-learning-based science, we propose REFORMS (Reporting standards for Machine Learning Based Science). It is a checklist of 32 items that can be helpful for researchers conducting ML-based science, referees reviewing it, and journals where it is submitted and published. It was developed by a consensus of 19 researchers across computer science, data science, social sciences, mathematics, and biomedical research. It was later </span><a href="https://email.mg-d0.substack.com/c/eJxU0D2O5CAQxfHTmGwQFGDcAcEmvoZVQLUHGRuLj2n17VetTXbiJ_310wvYaS_17Z6V6OtFOZSTWHSzwqeJjJy0etHGSAOMTkx52-miip3ihv2_VS2afbslzDKQNUEK_QQF9qEXG_xDzCAUzp4lBwKMMLBIax5ScsUJ0QhjhLeLXEwEXg57-0P6SYtz_4qCt-Fbx3DwUE6W2vaxfiyu10Esu-_e7zapPxOsE6yv14u3kOgKxEvdJ1hjSROsUnApYZ5gbSFh_OEYD6UNsHv4LZTzHFfq740u9Jniv_Y9fE4BeyrXlqKTQixCKFadz2VvkxaV7pyo8YC9HSNnflFnbfhYTkyXw9QuPKikzPrvi0ej-kkqbZSRFiz7cfA3AAD___Utf8Y" rel="" style="color: #fd6752;text-decoration: none;">published</a><span> in the journal Science Advances.</span></p><h4 class="header-anchor-post" style="position: relative;--highlight-bg: rgba(var(--color-bg-accent-themed-rgb), 0.2);font-family: 'SF Pro Display',-apple-system-headline,system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';font-weight: bold;-webkit-font-smoothing: antialiased;-moz-osx-font-smoothing: antialiased;-webkit-appearance: optimizelegibility;-moz-appearance: optimizelegibility;appearance: optimizelegibility;margin: 1em 0 0.625em 0;color: rgb(54,55,55);line-height: 1.16em;font-size: 1.125em;"><strong>4. AI and public policy</strong></h4><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">The two of us are at Princeton’s Center for Information Technology Policy. We regularly contribute our technical expertise to AI policy debates.&nbsp;</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0E2upSAQBeDVyEwDaKlvwKAndxumgNJbEcHw0zdv9x3Tg-43rtQ5X47DSkfK32bPRP2HgksXCW_mEXfwgoxapnUCUKAFXchhOyhSxkp-w_rfdVwn8TZEu1a7Bj_b9UvBDA6X3c8wrzBrNWvBRksNEvSqFvhSahgHQgQJIO2yqhW8HtK53PZUtpvkdfReDqXZUtGdg0uX4LI91sdiam4kgnnXepdu_NXpV6dfn89nQC4RT0ocnp9Ov-5OvwI7ioXj0XPpI3F9U-53wsI2UB9TFnezm0vX1SLX740i2kD-b8vdbGCHlVPc2Bsl5SrlKLKxIR2lm2SmOzCVwWEtZwthiFRFadanCzmafyJRf47dCuUncpxgBLXoRfw2-k8AAAD__01hhqI" rel="" style="color: #fd6752;text-decoration: none;">Licensing is neither feasible nor effective for addressing AI risks</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">Some people have advocated that only certain licensed companies and organizations should be allowed to build state-of-the-art AI models. We argue that licensing is infeasible to enforce because the cost of training models is dropping exponentially. Besides, licensing will increase market concentration, harming competition and worsening many AI risks.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0L2u3CAQBeCnMZ0RP55r34Iizb6GNcCwi4zB4ierffvISpHcejTnfDoOOz1L_ZhQieY3JVdOYt58aQzgGRm5LtsCIEExOjGm_UmZKnbyO_b_rnpb2Mt4JwMJ7-USSAf4hvULELaAChGtUCwaJRQIUJtc4VtKrjkhggAQdt3kBl7xcqyXPaSdFnE-Zy94G7Z1dAd35WSx7bf1tpheB7FkXr1fbdK_JvWY1OP9fnOMLeNBJab7Z1KPa1IPnBsGml9YbalzKHWO2dNF2VPuM0Z2Dbu7cp4jx_7ZKaNN5P-WXMOm6LDHkvfojRRiE0KzamwqzzYtotKVIjXusLdjpMQzddaG9eXEmM0_EOs_tx6N6h2pF9AgV7Wy30b9CQAA__-5yobg" rel="" style="color: #fd6752;text-decoration: none;">A safe harbor for AI evaluation and red teaming</a><br></strong><em>By Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Arvind Narayanan, Percy Liang, and Peter Henderson</em></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>Independent evaluation of AI is crucial for uncovering vulnerabilities, but this is often prohibited by AI companies’ Terms of Service, or left vague. This has a chilling effect on research. We were part of a group of researchers calling for change through a </span><a href="https://email.mg-d0.substack.com/c/eJxUkMuK5SAURb8mzkqOr-gdOOjJ_Y1wjCcpiYnBR3XX3zehJ12zDRsWi7Vip73Ub79Voo_flNdyEot-VriZyMgLq502RhjJ6MSUl50uqtgpLtj_e5XT7NNLEaxw84yb3hTN5Kxx6uUQAkEIDljyEqQBI52w5iUEV5wQDRgDwTrhTJS8HPYOhwiThnP_iMDbCK3jevC1nCy15XF9XHyvg1j2n73fbVK_Jvme5Bvrn_TFS92fHdok31KD4qDdS7F7hGUt5zmu1L8XujBkiv849wg5rdhTuZYUvQBwAIpVH3LZ26Sh0p0TNb5ib8fImV_UWRshlhPT5TG1Cw8qKbP-M-doVB-k0kYZYaVlX17-DQAA__-5S3lJ" rel="" style="color: #fd6752;text-decoration: none;">paper</a><span> and an </span><a href="https://email.mg-d0.substack.com/c/eJxU0DvO3CAUxfHVmM4IMBhPQZHG27AucO1B5mHxSDS7j0Zp8tVH-uun46DjVerHnBVx_oPRlYTEm3WBU3mChmu5SaW4EgQThHhcmLFCR39A_29dNkneBk-58pVt9uX8iqfkwBk4puVqNXfCk2AEE4opsXGtXpzThSKAYkoxqze-KS9oufVjb24nydI1e0bbsK2Du6kriYR2fK1fi-l1IInm3fvTpuXXJPZJ7C10bDSFTtGPSewQ5gYnzm-ottRJ7OQZ9nAlpZFD_xyYwUb0_2LPsDE46KHkI3jDGdsYW0g1NparTZJVfGLARh30do8YacZO2rC-JAjZQGgZbiwhkv7z09GwfpOLVIviWmjy24i_AQAA__94kn0O" rel="" style="color: #fd6752;text-decoration: none;">open letter</a><span>. The effort has been impactful both in public policy and in getting companies to make changes to their own policies.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0MvO2yAQBeCnMTsjrsFesOgmr2ENMHGQMVhcEuXtq6iV2n89mnM-HQ8d91I_9lER5zcmX04kwd4kPHQgaLlRi9Kaa0HwhJi2HTNW6Bg26P9d5aLI08qb0KtYQSklzcpv8mGcgvWBAhRfF0aiFUxopsXCjV45p5IigGZaM2cWvuggaDnM5Q7uJsXOfQ6MtuFaB39QX04S2_a1fi2214Ek2WfvV5vkr0ncJ3F_v98UYstwYInp-zOJ-zWJ-195fOEMsc2Yw1xHnqGWkcPsy_WpcX92cg23-XKeI8f-2TCDSxj-dF3Dpeihx5K3GCxnbGFMkmpdKnubFKt4pYiNeujtGCnRjJ204UI5IWb7z0X6z8lHw_qNlEpLzY0w5GXF7wAAAP__3cmIgA" rel="" style="color: #fd6752;text-decoration: none;">Generative AI’s end-run around copyright won’t be resolved by the courts</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">The New York Times’ lawsuit against OpenAI is filled with examples of ChatGPT outputting near-verbatim copies of text from the NYT. But output similarity is almost totally disconnected from what is ethically and economically harmful about generative AI companies’ practices. As a result, the lawsuit might lead to a pyrrhic victory. It would allow generative AI companies to proceed without any significant changes to their business models.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkM2O3SAUg58m7BLxO9AFi27ua0QHOLmDQiAKh4nm7auoldquLdufHYHw3a5vv12I840ltgNZ8h8KNpMYemG108YIIxkekMv6xooXEKYV6B9VOc0-PRdb2D5c1NFyK3TagjUpaK2UQGe5Y9lLLg030glrfgixqAUBDDeGB-uEM0kubbdn2EWYND_ec-JLH6ETxH2J7WC5rw_rw-LpGsiK_yQ6-6R-TvI1ydd93wvkXmHHlsvjmeTrnOQLLsqd-hyhzrXdcztpboPmts1_RuUvZOcIa2zHMWqm7xUrhILpd9M5QskRKLe65uQF545zxS4fSnv3SfMLz5KxLxGo76OUpSKxPkJqB-Tq_1Ix-v_w0fF6IpU2yggrLfvy8lcAAAD__5h1iEY" rel="" style="color: #fd6752;text-decoration: none;">Artists can now opt out of generative AI. It’s not enough.</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">We argue that generative AI companies cannot do right by artists without changing their business model, and give many examples of how AI companies externalize the costs of their products onto others.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0E3upCAQBfDTyE5TIDS6YDGbvoYpoNomIhg-ptO3n5hZzPzXlXrvl-ew0Z7L17wK0fih6PJJzJvHjC_lGRmu5SKV4kowOjHEbadEBRv5Ddt_13mR7G3kSwuN-rEACK5n-7AeBDhaVyUB15UFI0AoUGLhWq2cT_NEiAqUAqsXvigvpnzoyx7cDhLOffQw1W5rQ3dMLp8s1O223hbTSicWzbu1qw7zr0E8B_H8fD4ThprwoBzi_TOI5zWIZyP3Hq8cg_uOoY45xe_4Kr22gi2kfVyBXd1uLp9nT6F9N0poI_m_NVe3MThsIacteMMBFoCZFWNj3usgodAVA9XJYatHj3FK1Fjt1ucTQzL_SKz9XLtXKnfkLNWsuBaa_TbiTwAAAP__FhWGQw" rel="" style="color: #fd6752;text-decoration: none;">Tech policy is only frustrating 90% of the time</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">We think there is nothing exceptional about tech policy that makes it harder than any other type of policy requiring deep expertise.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkDuupDAURFeDM5A_3IYXOJikt4Gu7Uu3hT_In8f07kdogpkXH1XVUVls9Mrlo_dCNF4UbI7EnH4o3MEx0mKZ1xlAgGQU0YftRYkKNnIbtv-oWmf21qAcGgI7r9btzi6Oix2WXT7U1zo_0DCvJZfAQa5igS8hJjURInAAbpZVrODklI_lNIcww8zja3R8qt3UhvaYbI7M1-12vV10K51Y0O_WzjqoX4N8DvJ5XdeEviY8KPtwZwb5PG_wxja2N430m2xv_pvGXByVMRKmOu65sLObzeYYe_Lts1FCE8j9nTm7Cd5i8zlt3mnB-cq5YkWbkF91mHmhM3iqk8VWjx7ClKix2o3LEX3S_5RY-_l2r1TuSjWDArHIhX1r-ScAAP__6iOH4w" rel="" style="color: #fd6752;text-decoration: none;">What the executive order means for openness in AI</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">The 2023 Executive Order on AI is 20,000 words long and tries to address the entire range of AI benefits and risks. It is likely to shape every aspect of the future of AI, including openness: Will it remain possible to publicly release model weights while complying with the EO’s requirements? How will the EO affect the concentration of power and resources in AI? What about the culture of open research? It’s good news on paper, but the devil is in the details.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxUkMvOpCAQhZ9GdhIuovaCxWx8DVNAaRMRDJc2_fYTM5PM_OtKnfN9x0LFPeWv3jJif2Ow6UTi9ChhU46g5tMwD0pxJQie4MO6Y8QMFd0K9b-rnAfy1kohSgd8E_I1AIwwbszi5LgZhUOHxGvBhGJKzHxSL86ppAigmFLMTDOflRM0HdNlDm66gZ177xgtzZQK9qA2ncSX9WF9WHTNDUnQ71qv0slfnVg6sdz3TcGXCAcmH56fTixXJ5b6fhy9Qyj9lnKfcW8Bqo97_1fKf5Bczaw2nWeLvn5XjGACuj9NVzPBW6g-xdU7zRmbGZMkaxPSXrqBZbyCx0It1HK0EGjESkozLp3go_5HRerPwVvB_ETKQUnFJzGRjxa_AwAA__8AcomK" rel="" style="color: #fd6752;text-decoration: none;">Three Ideas for Regulating Generative AI</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;">We were part of a Stanford-Princeton team providing policy input to the federal government. We advocate for transparency, holistic public evaluations, and guardrails for responsible open-source AI research and development.</p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><strong><a href="https://email.mg-d0.substack.com/c/eJxU0Emu3CAQBuDTmJ0R4zO9YJFNX8MqoNqNzGAZeFbfPnISKcn6r-HT76HjVs-PfZ2I84XJ14wk2C8JLx0IWr4oo7TmWhDMENO6YcETOoYV-j-pNIq87Zd-SDAopOeKg5b-zpcgDD7U48UZiVYwoZkWhi_6wTmVFAE005q5xXCjg6B1Xw63czcplrc5MNqGax38Tn3NJLb1tt4W28-BJNl370eb5I9JPCfxvK6LQmwFdqwx3TuTeB6TeP6Rx2-cIc6-5gNKxDbn0fp8DJdie5NjuNXXnEeJ_bNiAZcw_H70a8RDj7WsMVjOmGFMktO6VLc2KXbikSI26qG3faREC3bShgs1Qyz2L4r0__seDc_7pFRaar6IhXxb8TMAAP__iSSHnw" rel="" style="color: #fd6752;text-decoration: none;">Generative AI companies must publish transparency reports</a></strong></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;"><span>We draw on the parallels between social media and generative AI, and call for transparency on how these tools are being used. Later we coauthored a </span><a href="https://email.mg-d0.substack.com/c/eJxUkEvK5CAURlcTZyW-YwYOelLbCFe9lZKYGHxUd-2-CT3pf_bBB4fDCdBxK_XrXhXx8RtzKAeS6IyEl44EHZ-VVVpzLQgekPK64YkVOsYV-n-vtIq8XTRKm1eMJixoMRpmBFN2WVAKH5YZSXKCCc20sHzWC-dUUgTQTGvmZ8utjoKWfb78zv2k2LE9IqNt-NYh7DSUg6S23q63i-t1IMnu3fvVJvlrEs9JPKH-SR9a6nZv3ybxFIoJyo0wllzDr6EcxzhT_654gs8Y_3Gu4XMK0FM51xQdZ8wyJkl1PpetTYpVvHLCRgP0to-c6YmdtOFjOSCdDlI7YceSMuk_c46G9UZKpaXms5jJx4m_AQAA__8K-HmN" rel="" style="color: #fd6752;text-decoration: none;">paper</a><span> (AIES 2024) with Stanford and MIT researchers in which we put forth a much more comprehensive blueprint for transparency.</span></p><p style="margin: 0 0 20px 0;color: rgb(54,55,55);line-height: 26px;font-size: 16px;margin-bottom: 0;"></p></div></div><div class="footer footer-ZM59BM" style="color: rgb(119,119,119);text-align: center;font-size: 16px;line-height: 26px;padding: 24px0;"><div style="font-size: 16px;line-height: 26px;padding-bottom: 24px"><p class="pencraft pc-reset color-secondary-ls1g8s size-12-mmZ61m reset-IxiVJZ small meta-B2bqa5" style="list-style: none;font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';padding-bottom: 0;font-size: 12px;line-height: 16px;margin: 0;color: rgb(119,119,119);text-decoration: unset;">© 2025 <span>Sayash Kapoor and Arvind Narayanan</span><br>Princeton University, NJ 08544 <br><a href="https://email.mg-d0.substack.com/c/eJxUks1uozwUQJ8G74r8g4FvwSJfUyqqIVE6aZPpBvmPxMXYCJu4ydOPMrOYmfXVPTr36AoW1MnN16qflXqIygg3KiCrnLCeSqAqVGRlRimiGKiRadOdlFUzC0p2LPw1JWUGzlXGaV5QWEKK-l4IjnKIeS4zKiWjWAmgKwwxhRSXqKD_IZSSVDFGIaWQFyUqqcSpG4qJD4gnGRxPDxKmfuE-MDGkwo1A--7uenepwrwoYKpzCJNPyCrBdYLrGGPKtLdsUE6b-06CayaCdjbBtdSecaO6X4CE1MENyiZkra4vSOD36xGbofl0t8367bbZP5H2U_jG_n8R5LVnh53ejhTx5-ib0ZzlY5O3ewG36x3a7J9Qq6P-OJ5j8-m-NjdBt_vhq92_Xb89vkyCtHqrX6I8NKHdr-J2vYrttfGN3aAfusmb8XUSuNb8-b3_OKAzO0Td71KTE3sg8Xtd346dX5UXshs2EGVPQ_joMezq0y5YOOL3YwmmhXfCjeNidbh2yt7vlL8bTQs3WrB7g07LCkFYQkjAXHHjTj7J4Kwmo5VPBQt-WIxJrQrAL1y6kWlb_ekJwr-vsng135Eko4SiAhfgUuGfAQAA__9F18aO" style="color: #fd6752;text-decoration: none;"><span style="color: rgb(119,119,119);text-decoration: underline;">Unsubscribe</span></a></p></div><p class="footerSection-EHR0jG small powered-by-substack" style="padding: 0 24px;font-size: 12px;line-height: 20px;margin: 0;color: rgb(119,119,119);font-family: system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Helvetica,Arial,sans-serif,'Apple Color Emoji','Segoe UI Emoji','Segoe UI Symbol';padding-bottom: 0;margin-top: 0;"><a href="https://email.mg-d0.substack.com/c/eJxUkUuO7CoMQFcDs0R8QpIaMHhSq7YRGXClUfhEYN5V7_6quid9J56cI_tI9kB41vZlXw1x-oPJ14w82FXDywSOVm7LvhgjjeKYIabjxIINCMMB9IvqfeGf9gG7FGY1oI1WEkGsWuBjh2VdtF_XF49WCWWEUbvczEPKWc8IYIQxwm273E1Qc722213SsUXkcwpi7sN1An_NvmYe-_FufbdYagN5sp9Ed2f6P6aeTD1_20w94b5_5tSpNpwahtjQE9PPQfnwkG-IZ2H64-1kaBdSLCdT6zeuhbAQ0x_fF6dXrYRtcoOoFn4Pd_ia8yiRvg4s4BKGn6p7uBQ9UKzliMFKIXYhNG_WpXp2toiGd4rYZw_Ur5HSXJB4Hy7UDLFYiL3AhTUmTv8-Z3Rs75V6MdrITW38f6v-BgAA__-ki5fG" style="color: #fd6752;text-decoration: none;display: inline-block;margin: 0 4px;"><img src="https://substackcdn.com/image/fetch/w_262,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fgeneric-app-button%402x.png" srcset="https://substackcdn.com/image/fetch/w_131,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fgeneric-app-button.png, https://substackcdn.com/image/fetch/w_262,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fgeneric-app-button%402x.png 2x, https://substackcdn.com/image/fetch/w_393,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fgeneric-app-button%403x.png 3x" width="131" alt="Get the app" height="40" style="max-width: 550px;border: none !important;vertical-align: middle;"></a><a href="https://email.mg-d0.substack.com/c/eJx8kUuO3SAQRVcDs7YwGOMeMIjkvA1kAVYZCjcxH4tPot595Jd01JlkyOXq1CmVgYZHLu_aFcSXnxhMjkitngU4aSnqUU3LJOUoOcUIPmwHJizQ0G7QPv2KZaJvGpiZX5Vy0vGZ2cWqWS5OGo6GWeUWR73mjEsm-TIq-TqOgxgQQDIp2a6WcZGWD_lU136OO5lYPF4sG2rfawNzDiZH6ut2u94uupWONOi31q5KxBfCH4Q_Prfvpz9Sv4h49Ba3mnsxSMT6USJ8vvOI1vdIxPrk_glNTg1TI2J1OTcsHzHEC_yRiFiht-x8CGhf_lZuuW_PmV-fLLHuIR-VTKzgFTzWwUCrZw9hSNgInwsRq7yiEN_p1ffN5Bh78u19wwR7QPt7y6vvwRtoPqfNWz0ytjAmaNH_o9Pad5sj-KTB1wQnZh9o-_fYvWK5kWKSQo6KK_pD818BAAD__-24tJ4" style="color: #fd6752;text-decoration: none;display: inline-block;margin: 0 4px;"><img src="https://substackcdn.com/image/fetch/w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png" srcset="https://substackcdn.com/image/fetch/w_135,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button.png, https://substackcdn.com/image/fetch/w_270,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%402x.png 2x, https://substackcdn.com/image/fetch/w_405,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Femail%2Fpublish-button%403x.png 3x" width="135" alt="Start writing" height="40" style="max-width: 550px;border: none !important;vertical-align: middle;"></a></p></div></div></td><td></td></tr></tbody></table><img src="https://eotrx.substackcdn.com/open?token=eyJtIjoiPDIwMjUwNTI4MTc1OTExLjMuZWFhNTA1NTBiNzgxODVkMi5vazdwYmsxYkBtZy1kMC5zdWJzdGFjay5jb20-IiwidSI6MzQ1MzUxNzI3LCJyIjoiYmxvZ3NAcmVwbGllcy5jYXRza3VsbC5uZXQiLCJkIjoibWctZDAuc3Vic3RhY2suY29tIiwicCI6bnVsbCwidCI6bnVsbCwiYSI6bnVsbCwicyI6MTAwODAwMywiYyI6ImZyZWUtd2VsY29tZSIsImYiOnRydWUsInBvc2l0aW9uIjoiYm90dG9tIiwiaWF0IjoxNzQ4NDU1MTUyLCJleHAiOjE3NTEwNDcxNTIsImlzcyI6InB1Yi0wIiwic3ViIjoiZW8ifQ.91A_MOCqj0GFABIUBx1PchYsaXb8qhRebB7PBj8uRys" alt="" width="1" height="1" border="0" style="height:1px !important;width:1px !important;border-width:0 !important;margin-top:0 !important;margin-bottom:0 !important;margin-right:0 !important;margin-left:0 !important;padding-top:0 !important;padding-bottom:0 !important;padding-right:0 !important;padding-left:0 !important;"><img width="1" height="1" alt="" src="https://email.mg-d0.substack.com/o/eJxU0EuuGyEQheHVmJlbBXQZPGAtrQLKHdQ8LB6JvPvIkyh3_EtHn06gyWfrH_fqzPc_nEMrLKJ7aHphFOyk2e2OKFEJLpTycXLlTpPjQfO_qu0ufrnAxMoSkjQeUAfpPQA9gqQnEjCJ5BQoBFRWGnxKuemNiRAQwRsrLUa1tcu8_SX9bYdy3iNsY_kxKVxbaEWkcXytX4ubfbF4L3-EVsqqaX4OruQzx38pp0AztXqk6CSABdCiO5_bOW47dH7nxGMLNMe1ct4qTzGWj61Qqo7SqHRxS1nMnw-twf07qXfUKI0y4rdTfwMAAP__6ARtJg">

